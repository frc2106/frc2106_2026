// Copyright (c) 2025 - 2026 : FRC 2106 : The Junkyard Dogs
// https://www.team2106.org

// Use of this source code is governed by an MIT-style
// license that can be found in the LICENSE file at
// the root directory of this project.

package frc.robot.lib.windingmotor.vision;

import static frc.robot.constants.LIB_VisionConstants.*;

import edu.wpi.first.math.Matrix;
import edu.wpi.first.math.VecBuilder;
import edu.wpi.first.math.geometry.Pose2d;
import edu.wpi.first.math.geometry.Pose3d;
import edu.wpi.first.math.geometry.Rotation2d;
import edu.wpi.first.math.numbers.N1;
import edu.wpi.first.math.numbers.N3;
import edu.wpi.first.wpilibj.Alert;
import edu.wpi.first.wpilibj.Alert.AlertType;
import edu.wpi.first.wpilibj2.command.SubsystemBase;
import frc.robot.constants.LIB_VisionConstants;
import frc.robot.lib.windingmotor.vision.IO_VisionBase.PoseObservationType;
import java.util.LinkedList;
import java.util.List;
import java.util.Set;
import org.littletonrobotics.junction.Logger;

/**
 * Vision subsystem that processes camera observations and provides pose estimates to the drive
 * subsystem.
 *
 * <p>Key responsibilities: - Polls all camera IO implementations for new data - Filters pose
 * observations based on quality metrics - Calculates appropriate standard deviations for each
 * observation - Provides pose estimates to the drive subsystem's pose estimator - Logs
 * comprehensive telemetry for debugging and replay
 *
 * <p>Filtering criteria: - Must have at least one tag - Single-tag observations must have ambiguity
 * below threshold - Z-coordinate must be realistic (robot can't be in the air) - Pose must be
 * within field boundaries - Only allowed tag IDs are processed (security measure)
 *
 * <p>Standard deviations scale with tag distance and count: - More tags = lower uncertainty -
 * Closer tags = lower uncertainty - MEGATAG_2 gets higher uncertainty than PHOTONVISION -
 * Per-camera factors allow individual camera tuning
 */
public class SUB_Vision extends SubsystemBase {
	/** Consumer that accepts filtered pose estimates for the drive subsystem. */
	private final VisionConsumer consumer;

	/** Array of camera IO implementations (one per physical camera). */
	private final IO_VisionBase[] io;

	/** Array of input objects for each camera (auto-generated by AdvantageKit). */
	private final VisionIOInputsAutoLogged[] inputs;

	/** Alerts for disconnected cameras. */
	private final Alert[] disconnectedAlerts;

	/**
	 * Constructs the vision subsystem with multiple cameras.
	 *
	 * @param consumer Callback that receives filtered pose estimates
	 * @param io Variable number of camera IO implementations
	 */
	public SUB_Vision(VisionConsumer consumer, IO_VisionBase... io) {
		this.consumer = consumer;
		this.io = io;

		// Initialize inputs array with auto-logged instances
		this.inputs = new VisionIOInputsAutoLogged[io.length];
		for (int i = 0; i < inputs.length; i++) {
			inputs[i] = new VisionIOInputsAutoLogged();
		}

		// Initialize disconnected alerts for each camera
		this.disconnectedAlerts = new Alert[io.length];
		for (int i = 0; i < inputs.length; i++) {
			disconnectedAlerts[i] =
					new Alert(
							"Vision camera " + Integer.toString(i) + " is disconnected.", AlertType.kWarning);
		}
	}

	/**
	 * Gets the horizontal angle to the best target for simple vision servoing.
	 *
	 * @param cameraIndex Which camera to query (0 = first camera)
	 * @return The target angle, or zero if no target visible
	 */
	public Rotation2d getTargetX(int cameraIndex) {
		return inputs[cameraIndex].latestTargetObservation.tx();
	}

	/**
	 * Main periodic method that processes all camera data.
	 *
	 * <p>For each camera: 1. Updates inputs from IO layer 2. Processes all pose observations 3.
	 * Filters out low-quality observations 4. Calculates standard deviations based on distance and
	 * tag count 5. Sends accepted poses to the consumer (drive subsystem) 6. Logs comprehensive
	 * telemetry for each camera and a summary
	 */
	@Override
	public void periodic() {
		// Collect data from all cameras
		for (int i = 0; i < io.length; i++) {
			io[i].updateInputs(inputs[i]);
			Logger.processInputs("Vision/Camera" + Integer.toString(i), inputs[i]);
		}

		// Initialize logging collections
		List<Pose3d> allTagPoses = new LinkedList<>();
		List<Pose3d> allRobotPoses = new LinkedList<>();
		List<Pose3d> allRobotPosesAccepted = new LinkedList<>();
		List<Pose3d> allRobotPosesRejected = new LinkedList<>();
		List<Integer> allTagsUsed = new LinkedList<>();

		// Define allowed tag IDs (whitelist for security)
		// Only these tags are processed; others are ignored even if detected
		Set<Integer> allowedTagIds = LIB_VisionConstants.allowedTagIds;

		// Process each camera
		for (int cameraIndex = 0; cameraIndex < io.length; cameraIndex++) {
			// Update disconnected alert
			disconnectedAlerts[cameraIndex].set(!inputs[cameraIndex].connected);

			// Per-camera logging collections
			List<Pose3d> tagPoses = new LinkedList<>();
			List<Pose3d> robotPoses = new LinkedList<>();
			List<Pose3d> robotPosesAccepted = new LinkedList<>();
			List<Pose3d> robotPosesRejected = new LinkedList<>();
			List<Integer> tagsUsed = new LinkedList<>();

			// Add tag poses for detected tags (only allowed ones)
			for (int tagId : inputs[cameraIndex].tagIds) {
				if (allowedTagIds.contains(tagId)) {
					var tagPose = aprilTagLayout.getTagPose(tagId);
					if (tagPose.isPresent()) {
						tagPoses.add(tagPose.get());
						tagsUsed.add(tagId);
						allTagsUsed.add(tagId);
					}
				}
			}

			// Process all pose observations from this camera
			for (var observation : inputs[cameraIndex].poseObservations) {
				// Track which tags are in this observation
				List<Integer> observationTags = new LinkedList<>();

				// Check if observation contains any allowed tags
				boolean containsAllowedTag = false;
				for (int tagId : inputs[cameraIndex].tagIds) {
					if (allowedTagIds.contains(tagId)) {
						containsAllowedTag = true;
						observationTags.add(tagId);
					}
				}

				// Skip this observation entirely if it doesn't contain any allowed tags
				// This prevents processing observations that don't contain tags we're interested in
				if (!containsAllowedTag) {
					robotPoses.add(observation.pose());
					robotPosesRejected.add(observation.pose());
					continue;
				}

				// Quality filter: check whether to reject this pose observation
				// Rejection criteria (must pass ALL to be accepted):
				// 1. Must have at least one tag
				// 2. If single-tag, ambiguity must be below threshold (high ambiguity = unreliable)
				// 3. Z-coordinate must be realistic (robot can't hover in the air)
				// 4. Must be within field boundaries (can't be outside the field)
				boolean rejectPose =
						observation.tagCount() == 0 // Must have at least one tag
								|| (observation.tagCount() == 1
										&& observation.ambiguity() > maxAmbiguity) // Single-tag must have low ambiguity
								|| Math.abs(observation.pose().getZ())
										> maxZError // Z-coordinate must be realistic (robot not in air)

								// Must be within field boundaries
								|| observation.pose().getX() < 0.0
								|| observation.pose().getX() > aprilTagLayout.getFieldLength()
								|| observation.pose().getY() < 0.0
								|| observation.pose().getY() > aprilTagLayout.getFieldWidth();

				// Add pose to appropriate logging collection
				robotPoses.add(observation.pose());
				if (rejectPose) {
					// Rejected poses are logged for debugging but not sent to pose estimator
					robotPosesRejected.add(observation.pose());
				} else {
					// Accepted poses are sent to the pose estimator and logged separately
					robotPosesAccepted.add(observation.pose());
					// Log which tags were used in this accepted observation
					int[] observationTagsArray =
							observationTags.stream().mapToInt(Integer::intValue).toArray();
					Logger.recordOutput(
							"Vision/Camera"
									+ Integer.toString(cameraIndex)
									+ "/Observation"
									+ robotPosesAccepted.size()
									+ "/TagsUsed",
							observationTagsArray);
				}

				// Skip further processing if pose was rejected
				if (rejectPose) {
					continue;
				}

				// Calculate standard deviations (uncertainty) for this observation
				// The uncertainty increases with the square of average tag distance and decreases with tag
				// count
				// This means: closer tags = more accurate, more tags = more accurate
				double stdDevFactor =
						Math.pow(observation.averageTagDistance(), 2.0) / observation.tagCount();
				double linearStdDev = linearStdDevBaseline * stdDevFactor;
				double angularStdDev = angularStdDevBaseline * stdDevFactor;

				// MEGATAG_2 observations are typically less reliable than PHOTONVISION
				// Apply additional scaling factor for this observation type
				if (observation.type() == PoseObservationType.MEGATAG_2) {
					linearStdDev *= linearStdDevMegatag2Factor;
					angularStdDev *= angularStdDevMegatag2Factor;
				}

				// Apply per-camera standard deviation factors if configured
				// This allows tuning individual cameras that may have different reliability
				if (cameraIndex < cameraStdDevFactors.length) {
					linearStdDev *= cameraStdDevFactors[cameraIndex];
					angularStdDev *= cameraStdDevFactors[cameraIndex];
				}

				// Send the accepted vision observation to the drive subsystem's pose estimator
				// The standard deviations allow the estimator to weight this measurement appropriately
				consumer.accept(
						observation.pose().toPose2d(), // Convert to 2D for pose estimator
						observation.timestamp(), // Timestamp for interpolation
						VecBuilder.fill(linearStdDev, linearStdDev, angularStdDev)); // Standard deviations
			}

			// Log all camera data for debugging and replay
			// These logs are invaluable for diagnosing vision issues and tuning filters
			Logger.recordOutput(
					"Vision/Camera" + Integer.toString(cameraIndex) + "/TagPoses",
					tagPoses.toArray(new Pose3d[tagPoses.size()]));
			Logger.recordOutput(
					"Vision/Camera" + Integer.toString(cameraIndex) + "/RobotPoses",
					robotPoses.toArray(new Pose3d[robotPoses.size()]));
			Logger.recordOutput(
					"Vision/Camera" + Integer.toString(cameraIndex) + "/RobotPosesAccepted",
					robotPosesAccepted.toArray(new Pose3d[robotPosesAccepted.size()]));
			Logger.recordOutput(
					"Vision/Camera" + Integer.toString(cameraIndex) + "/RobotPosesRejected",
					robotPosesRejected.toArray(new Pose3d[robotPosesRejected.size()]));

			// Convert tags used to primitive array for logging
			int[] tagsUsedArray = tagsUsed.stream().mapToInt(Integer::intValue).toArray();
			Logger.recordOutput(
					"Vision/Camera" + Integer.toString(cameraIndex) + "/TagsUsed", tagsUsedArray);

			// Add this camera's data to summary collections
			allTagPoses.addAll(tagPoses);
			allRobotPoses.addAll(robotPoses);
			allRobotPosesAccepted.addAll(robotPosesAccepted);
			allRobotPosesRejected.addAll(robotPosesRejected);
		}

		// Log summary data across all cameras
		// This provides a high-level view of vision performance
		Logger.recordOutput(
				"Vision/Summary/TagPoses", allTagPoses.toArray(new Pose3d[allTagPoses.size()]));
		Logger.recordOutput(
				"Vision/Summary/RobotPoses", allRobotPoses.toArray(new Pose3d[allRobotPoses.size()]));
		Logger.recordOutput(
				"Vision/Summary/RobotPosesAccepted",
				allRobotPosesAccepted.toArray(new Pose3d[allRobotPosesAccepted.size()]));
		Logger.recordOutput(
				"Vision/Summary/RobotPosesRejected",
				allRobotPosesRejected.toArray(new Pose3d[allRobotPosesRejected.size()]));

		// Convert all tags used to primitive array for logging
		int[] allTagsUsedArray = allTagsUsed.stream().mapToInt(Integer::intValue).toArray();
		Logger.recordOutput("Vision/Summary/TagsUsed", allTagsUsedArray);
	}

	/**
	 * Functional interface for consuming filtered pose estimates.
	 *
	 * <p>Implemented by the drive subsystem to receive vision measurements. The method signature
	 * matches WPILib's pose estimator requirements.
	 */
	@FunctionalInterface
	public static interface VisionConsumer {
		public void accept(
				Pose2d visionRobotPoseMeters,
				double timestampSeconds,
				Matrix<N3, N1> visionMeasurementStdDevs);
	}
}
